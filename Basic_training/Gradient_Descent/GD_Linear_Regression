import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import matplotlib.animation as animation

data = pd.read_csv('train.csv')


x = data['GrLivArea']
y = data['SalePrice']

x = (x - x.mean()) / x.std()   # standardization
x = np.c_[np.ones(x.shape[0]), x]  # combine b term into x

#GRADIENT DESCENT

alpha = 0.01 #Step size
iterations = 2000 #No. of iterations
m = y.size #Number of data points
np.random.seed(123) #Set the seed
theta = np.random.rand(2) #Pick some random values to start with

def gradient_descent(x, y, theta, iterations, alpha):
    past_costs = []
    past_thetas = [theta]
    for i in range(iterations):
        prediction = np.dot(x, theta)
        error = prediction - y
        cost = 1/(2*m) * np.dot(error.T, error) # MSE
        past_costs.append(cost) # MSE in each iteration
        theta = theta - (alpha * (1/m) * np.dot(x.T, error)) # b and w in each iteration
        past_thetas.append(theta)
        
    return past_thetas, past_costs

#Pass the relevant variables to the function and get the new values back...
past_thetas, past_costs = gradient_descent(x, y, theta, iterations, alpha)
theta = past_thetas[-1]

#Print the results...
print("Gradient Descent: {:.2f}, {:.2f}".format(theta[0], theta[1]))

# theta[0] is b and theta[1] is w 

plt.title('Cost Function J')
plt.xlabel('No. of iterations')
plt.ylabel('Cost')
plt.plot(past_costs)
plt.show()


# Copyright: From Rob Harrand in Kaggle, thanks for help
# https://www.kaggle.com/tentotheminus9/linear-regression-from-scratch-gradient-descent?select=train.csv
